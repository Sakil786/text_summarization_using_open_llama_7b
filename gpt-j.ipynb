{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f350c-4b7e-4ed8-99f5-a61bae9f1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5289d30-1fbe-4da2-8956-a3e73e89ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccc077-51d9-4720-96d4-3c361b3e3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e615557-480d-4839-b9dd-0ef33b48b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90822e4-f82d-40e0-b9ed-c8997cfac33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdb6c1e-a042-472f-ad01-476320dd589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-27): 28 x GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16,low_cpu_mem_usage=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389a5479-c58b-4852-8c1c-764626b6d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95acdf4-35c6-4f21-96b0-03a5e551bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"The Belgian national football team \"\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# generated_ids = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)\n",
    "# generated_text = tokenizer.decode(generated_ids[0])\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57a662-3b08-409d-9098-288a285533fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"\n",
    "# Sentence: This movie is very nice.\n",
    "# Sentiment: positive\n",
    "\n",
    "# #####\n",
    "\n",
    "# Sentence: I hated this movie, it sucks.\n",
    "# Sentiment: negative\n",
    "\n",
    "# #####\n",
    "\n",
    "# Sentence: This movie was actually pretty funny.\n",
    "# Sentiment: positive\n",
    "\n",
    "# #####\n",
    "\n",
    "# Sentence: This movie could have been better.\n",
    "# Sentiment: \"\"\"\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# generated_ids = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)\n",
    "# generated_text = tokenizer.decode(generated_ids[0])\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1409e59-121d-4834-8d0b-dc36d41d38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"\n",
    "# text: Question answering (QA) systems are a type of natural language processing (NLP) technology that provide\n",
    "# precise and concise answers to questions posed in natural language. These systems have the potential to\n",
    "# revolutionize the way we access information and can be applied in a wide range of fields including education,\n",
    "# customer service, and health care.There are several approaches to building QA systems, including rule-based,\n",
    "# information retrieval, and machine learning-based approaches. Rule-based systems rely on predefined rules and\n",
    "# patterns to extract answers from a given text, while information retrieval systems use search algorithms to\n",
    "# retrieve relevant information from a large database. Machine learning-based systems, on the other hand, use\n",
    "# training data to learn to extract answers from text.One of the main challenges faced by QA systems is the need\n",
    "# to understand the context and intent behind a question. This requires the system to have a deep understanding of\n",
    "# the language and the ability to make inferences based on the given information. Another challenge is the need to\n",
    "# extract relevant information from a large and potentially unstructured dataset.Despite these challenges, QA\n",
    "# systems have a wide range of applications, including education, customer service, and health care. In education,\n",
    "\n",
    "# summary: Question answering (QA) systems are a type of natural language processing (NLP) technology that provide precise and concise answers to questions posed in natural language. rule-based,\n",
    "# information retrieval, and machine learning-based are different approaches in building QA systems.One of the main challenges faced by QA systems is the need\n",
    "# to understand the context and intent behind a question. Another challenge is the need toextract relevant information from a large and potentially unstructured dataset. QA\n",
    "# systems have a wide range of applications, including education, customer service, and health care. By understanding the current state of development and the potential impact of QA systems, we can better utilize these technologies to\n",
    "# improve various industries and enhance the way we access information.\n",
    "\n",
    "# #####\n",
    "\n",
    "# Sentence: Most of us probably toss the word statistics around to bolster any random argument we make, but can we actually define it? According to Oxford Languages, statistics is â€œthe practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample\n",
    "\n",
    "# summary: \"\"\"\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# generated_ids = model.generate(input_ids, do_sample=True, temperature=0.5, max_length=200)\n",
    "# generated_text = tokenizer.decode(generated_ids[0])\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e6d9e-a64e-47d5-a5af-1fe04f0a0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"\n",
    "# text: Question answering (QA) systems are a type of natural language processing (NLP) technology that provide\n",
    "# precise and concise answers to questions posed in natural language. These systems have the potential to\n",
    "# revolutionize the way we access information and can be applied in a wide range of fields including education,\n",
    "# customer service, and health care.There are several approaches to building QA systems, including rule-based,\n",
    "# information retrieval, and machine learning-based approaches. Rule-based systems rely on predefined rules and\n",
    "# patterns to extract answers from a given text, while information retrieval systems use search algorithms to\n",
    "# retrieve relevant information from a large database. Machine learning-based systems, on the other hand, use\n",
    "# training data to learn to extract answers from text.One of the main challenges faced by QA systems is the need\n",
    "# to understand the context and intent behind a question. This requires the system to have a deep understanding of\n",
    "# the language and the ability to make inferences based on the given information. Another challenge is the need to\n",
    "# extract relevant information from a large and potentially unstructured dataset.Despite these challenges, QA\n",
    "# systems have a wide range of applications, including education, customer service, and health care. In education,\n",
    "\n",
    "# give the summary\n",
    "#  \"\"\"\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# generated_ids = model.generate(input_ids, do_sample=True, temperature=0.5, max_length=200)\n",
    "# generated_text = tokenizer.decode(generated_ids[0])\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9657433-b0d2-473f-b33e-450b2cc3ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432b987-5698-4a37-bfee-2bdb910d8f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import PromptTemplate,  LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a3e2540-cf00-4c47-af1b-f7dfd22b7160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            def add_two_numbers(x, y):\n",
      "\n",
      "                if x:\n",
      "                    return x + y\n",
      "                else:\n",
      "                    return y + y\n",
      "            def foo():\n",
      "                return x == y\n",
      "            def bar():\n",
      "                return x + y == y\n",
      "\n",
      "            assert not hasattr(add_\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "            def add_two_numbers(x, y):\n",
    "\n",
    "         \"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "generated_ids = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892fa41e-bb80-4a6b-b76e-ab633c2e1058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            summarize the text:Question answering (QA) systems are a type of natural language processing (NLP) technology that provideprecise and concise answers to questions posed in natural language. \n",
      "            These systems have the potential torevolutionize the way we access information and can be applied in a wide range of fields including education,customer service, and health care.\n",
      "\n",
      "            However, this type of technology is not without its drawbacks. Traditional systems that focus solely on thedetections of a given question typically require a large training corpus of text and are prone to error.As such\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "            summarize the text:Question answering (QA) systems are a type of natural language processing (NLP) technology that provideprecise and concise answers to questions posed in natural language. \n",
    "            These systems have the potential torevolutionize the way we access information and can be applied in a wide range of fields including education,customer service, and health care.\n",
    "\n",
    "         \"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "generated_ids = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=150)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435f73db-2c0c-4f50-b8e0-606be342deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e829920-2367-46a6-8ca5-fe0262650b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16,low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec12ee33-f661-4d83-bd49-4b5812a3be4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1751/1439542139.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text-generation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0;31m# Impossible to guess what is the right tokenizer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m                 raise Exception(\n\u001b[0m\u001b[1;32m    869\u001b[0m                     \u001b[0;34m\"Impossible to guess which tokenizer to use. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                     \u001b[0;34m\"Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1b42c6-5c2f-4756-bac5-2f4a0a7bcee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1792/3255183890.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text-generation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPTJForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EleutherAI/gpt-j-6B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16,low_cpu_mem_usage=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403fc0e-72e0-4cfb-9aec-a0b8ee2c540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = generator(\n",
    "#                        \"def add_two_numbers(x, y):\", \n",
    "#                        do_sample=True, \n",
    "#                        top_k=10, \n",
    "#                        temperature=0.6, \n",
    "#                        max_length=64, \n",
    "#                        num_return_sequences=1\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd6410-1ac4-4072-8942-3b9c9a80ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in sentences:\n",
    "#     print(sentence[\"generated_text\"])\n",
    "#     print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
